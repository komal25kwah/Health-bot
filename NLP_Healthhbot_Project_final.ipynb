{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8376584,
          "sourceType": "datasetVersion",
          "datasetId": 4980751
        },
        {
          "sourceId": 8377890,
          "sourceType": "datasetVersion",
          "datasetId": 4981721
        },
        {
          "sourceId": 8378036,
          "sourceType": "datasetVersion",
          "datasetId": 4981821
        },
        {
          "sourceId": 8385744,
          "sourceType": "datasetVersion",
          "datasetId": 4987489
        }
      ],
      "dockerImageVersionId": 30715,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "df = pd.read_excel('/kaggle/input/dataset/dataset.xlsx' , engine='openpyxl')\n",
        "\n",
        "# Combining columns\n",
        "df[\"Question\"] = (\"Description: \" + df.Description.str.strip() + \"; Patient: \" + df.Patient.str.strip())\n",
        "\n",
        "# Droping Description and Patient columns\n",
        "df.drop(['Description', 'Patient'], axis=1, inplace=True)\n",
        "\n",
        "df.rename(columns={'Doctor': 'Answer'}, inplace=True)\n",
        "\n",
        "df = df[['Question', 'Answer']]\n",
        "# Displaying the first 5 entries of the DataFrame in columns\n",
        "display(df.head())"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-17T06:24:21.448116Z",
          "iopub.execute_input": "2024-05-17T06:24:21.448786Z",
          "iopub.status.idle": "2024-05-17T06:24:59.018820Z",
          "shell.execute_reply.started": "2024-05-17T06:24:21.448752Z",
          "shell.execute_reply": "2024-05-17T06:24:59.017897Z"
        },
        "trusted": true,
        "id": "puRYwRZAY0hZ",
        "outputId": "c4da3e22-ee1a-437a-9079-3e3d4cef4bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                                            Question  \\\n0  Description: Q. What does abutment of the nerv...   \n1  Description: Q. What should I do to reduce my ...   \n2  Description: Q. I have started to get lots of ...   \n3  Description: Q. Why do I have uncomfortable fe...   \n4  Description: Q. My symptoms after intercourse ...   \n\n                                              Answer  \n0  Hi. I have gone through your query with dilige...  \n1  Hi. You have really done well with the hypothy...  \n2  Hi there Acne has multifactorial etiology. Onl...  \n3  Hello. The popping and discomfort what you fel...  \n4  Hello. The HIV test uses a finger prick blood ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Description: Q. What does abutment of the nerv...</td>\n      <td>Hi. I have gone through your query with dilige...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Description: Q. What should I do to reduce my ...</td>\n      <td>Hi. You have really done well with the hypothy...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Description: Q. I have started to get lots of ...</td>\n      <td>Hi there Acne has multifactorial etiology. Onl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Description: Q. Why do I have uncomfortable fe...</td>\n      <td>Hello. The popping and discomfort what you fel...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Description: Q. My symptoms after intercourse ...</td>\n      <td>Hello. The HIV test uses a finger prick blood ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "IwgTJzGUY0hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def decontractions(phrase):\n",
        "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
        "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
        "\n",
        "    return phrase\n",
        "\n",
        "def preprocess(text):\n",
        "    if isinstance(text, str):\n",
        "\n",
        "    # Converting all the text into lowercase\n",
        "    # Removing the words between brackets ()\n",
        "    # Removing these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
        "    # Replacing these special characters with space: '\\u200b', '\\xa0', '-', '/'\n",
        "\n",
        "        text = text.lower()\n",
        "        text = decontractions(text)\n",
        "        text = re.sub(r'\\(.*?\\)', '', text)  # Remove text between brackets\n",
        "        text = re.sub('[$)\\?\"’.°!;\\'€%:,(/]', '', text)\n",
        "        text = re.sub('\\u200b', ' ', text)\n",
        "        text = re.sub('\\xa0', ' ', text)\n",
        "        text = re.sub('-', ' ', text)\n",
        "        return text\n",
        "\n",
        "# Function to preprocess and clean the DataFrame\n",
        "def preprocess_dataframe(df):\n",
        "    # Applying preprocessing to the 'Question' column\n",
        "    df['Question'] = df['Question'].apply(preprocess)\n",
        "\n",
        "    # Applying preprocessing to the 'Answer' column\n",
        "    df['Answer'] = df['Answer'].apply(preprocess)\n",
        "\n",
        "    # Removing null values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Removing duplicates\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df = preprocess_dataframe(df)\n",
        "\n",
        "# Display the first 5 entries of the DataFrame\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:25:14.418182Z",
          "iopub.execute_input": "2024-05-17T06:25:14.418608Z",
          "iopub.status.idle": "2024-05-17T06:25:38.943790Z",
          "shell.execute_reply.started": "2024-05-17T06:25:14.418576Z",
          "shell.execute_reply": "2024-05-17T06:25:38.942911Z"
        },
        "trusted": true,
        "id": "9OtVua2JY0hb",
        "outputId": "011cf7be-38cf-42b9-e59b-431d82c807f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                                            Question  \\\n0  description q what does abutment of the nerve ...   \n1  description q what should i do to reduce my we...   \n2  description q i have started to get lots of ac...   \n3  description q why do i have uncomfortable feel...   \n4  description q my symptoms after intercourse th...   \n\n                                              Answer  \n0  hi i have gone through your query with diligen...  \n1  hi you have really done well with the hypothyr...  \n2  hi there acne has multifactorial etiology only...  \n3  hello the popping and discomfort what you felt...  \n4  hello the hiv test uses a finger prick blood s...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>description q what does abutment of the nerve ...</td>\n      <td>hi i have gone through your query with diligen...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>description q what should i do to reduce my we...</td>\n      <td>hi you have really done well with the hypothyr...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>description q i have started to get lots of ac...</td>\n      <td>hi there acne has multifactorial etiology only...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>description q why do i have uncomfortable feel...</td>\n      <td>hello the popping and discomfort what you felt...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>description q my symptoms after intercourse th...</td>\n      <td>hello the hiv test uses a finger prick blood s...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into features (X) and labels (y)\n",
        "X = df['Question']  # Assuming 'Question' column contains features\n",
        "y = df['Answer']    # Assuming 'Answer' column contains labels\n",
        "\n",
        "# Splitting the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Displaying the shape of the training and testing sets\n",
        "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:25:51.718949Z",
          "iopub.execute_input": "2024-05-17T06:25:51.719813Z",
          "iopub.status.idle": "2024-05-17T06:25:52.339761Z",
          "shell.execute_reply.started": "2024-05-17T06:25:51.719773Z",
          "shell.execute_reply": "2024-05-17T06:25:52.338664Z"
        },
        "trusted": true,
        "id": "kkZ23MROY0hc",
        "outputId": "0d9b779d-d1be-4d7c-9ea7-71362a8a8d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Training set shape: (197228,) (197228,)\nTesting set shape: (49308,) (49308,)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install reportlab"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:25:59.551732Z",
          "iopub.execute_input": "2024-05-17T06:25:59.552626Z",
          "iopub.status.idle": "2024-05-17T06:26:13.892481Z",
          "shell.execute_reply.started": "2024-05-17T06:25:59.552591Z",
          "shell.execute_reply": "2024-05-17T06:26:13.891246Z"
        },
        "trusted": true,
        "id": "xQPvM1OTY0hc",
        "outputId": "d431803d-1d98-4215-92f6-a1ee87acbac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting reportlab\n  Downloading reportlab-4.2.0-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from reportlab) (9.5.0)\nCollecting chardet (from reportlab)\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nDownloading reportlab-4.2.0-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: chardet, reportlab\nSuccessfully installed chardet-5.2.0 reportlab-4.2.0\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "from IPython.display import display, FileLink\n",
        "from io import BytesIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_pdf_from_dataframe(df, pdf_file):\n",
        "    # Create a BytesIO buffer to hold the PDF data\n",
        "    pdf_buffer = BytesIO()\n",
        "\n",
        "    # Creating a PDF document\n",
        "    c = canvas.Canvas(pdf_buffer, pagesize=letter)\n",
        "    width, height = letter\n",
        "\n",
        "    # Setting initial y-coordinate for content\n",
        "    y = height - 50\n",
        "\n",
        "    # Looping through each row in the DataFrame and add content to PDF\n",
        "    for index, row in df.iterrows():\n",
        "        question = row['Question']\n",
        "        answer = row['Answer']\n",
        "\n",
        "        # Writing question and answer to PDF\n",
        "        c.drawString(50, y, \"Question: {}\".format(question))\n",
        "        y -= 20\n",
        "        c.drawString(50, y, \"Answer: {}\".format(answer))\n",
        "        y -= 30  # Adjust spacing between questions and answers\n",
        "\n",
        "\n",
        "        if y < 50:\n",
        "            c.showPage()\n",
        "            y = height - 50\n",
        "\n",
        "    # Saving\n",
        "    c.save()\n",
        "\n",
        "\n",
        "    pdf_buffer.seek(0)\n",
        "\n",
        "\n",
        "    with open(pdf_file, \"wb\") as f:\n",
        "        f.write(pdf_buffer.read())\n",
        "\n",
        "\n",
        "    display(FileLink(pdf_file))\n",
        "\n",
        "# Split the dataframe into training and testing sets\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save PDF files for training and testing sets\n",
        "create_pdf_from_dataframe(df_train, \"Training_Dataset.pdf\")\n",
        "create_pdf_from_dataframe(df_test, \"Testing_Dataset.pdf\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:26:17.470543Z",
          "iopub.execute_input": "2024-05-17T06:26:17.471547Z",
          "iopub.status.idle": "2024-05-17T06:29:15.386702Z",
          "shell.execute_reply.started": "2024-05-17T06:26:17.471504Z",
          "shell.execute_reply": "2024-05-17T06:29:15.385744Z"
        },
        "trusted": true,
        "id": "DZEdwLhRY0hc",
        "outputId": "ab184882-dd63-473c-ac04-be55adb1826b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "/kaggle/working/Training_Dataset.pdf",
            "text/html": "<a href='Training_Dataset.pdf' target='_blank'>Training_Dataset.pdf</a><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "/kaggle/working/Testing_Dataset.pdf",
            "text/html": "<a href='Testing_Dataset.pdf' target='_blank'>Testing_Dataset.pdf</a><br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:31:31.965507Z",
          "iopub.execute_input": "2024-05-17T06:31:31.966360Z",
          "iopub.status.idle": "2024-05-17T06:31:49.499567Z",
          "shell.execute_reply.started": "2024-05-17T06:31:31.966316Z",
          "shell.execute_reply": "2024-05-17T06:31:49.498217Z"
        },
        "trusted": true,
        "id": "HDDKeusuY0hc",
        "outputId": "ac6735fa-2046-464b-b9e5-3d07ebbc8696"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting langchain\n  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nCollecting langchain-community<0.1,>=0.0.38 (from langchain)\n  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\nCollecting langchain-core<0.2.0,>=0.1.52 (from langchain)\n  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.59-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nDownloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\nDownloading langsmith-0.1.59-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.2 langsmith-0.1.59 orjson-3.10.3 packaging-23.2\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyDrive"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:31:57.339877Z",
          "iopub.execute_input": "2024-05-17T06:31:57.340892Z",
          "iopub.status.idle": "2024-05-17T06:32:12.493651Z",
          "shell.execute_reply.started": "2024-05-17T06:31:57.340834Z",
          "shell.execute_reply": "2024-05-17T06:32:12.492404Z"
        },
        "trusted": true,
        "id": "DImtIYqOY0hc",
        "outputId": "ccee78a3-c4b0-4723-f7c5-3711b6d9ba8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting PyDrive\n  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /opt/conda/lib/python3.10/site-packages (from PyDrive) (2.126.0)\nRequirement already satisfied: oauth2client>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from PyDrive) (4.1.3)\nRequirement already satisfied: PyYAML>=3.0 in /opt/conda/lib/python3.10/site-packages (from PyDrive) (6.0.1)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (0.21.0)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (2.26.1)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (0.2.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (2.11.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (0.5.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (0.3.0)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (4.9)\nRequirement already satisfied: six>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (1.16.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.62.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.20.3)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.31.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.2->PyDrive) (4.2.4)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.2->PyDrive) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2024.2.2)\nBuilding wheels for collected packages: PyDrive\n  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyDrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27434 sha256=e10aff19a34c42d58c330d0fddc27a01bccc89a282bd3d46ab6120a1a0802908\n  Stored in directory: /root/.cache/pip/wheels/63/79/df/924c22c080c9dac1a57f611baa837fe0bc3daec1500b27f23b\nSuccessfully built PyDrive\nInstalling collected packages: PyDrive\nSuccessfully installed PyDrive-1.3.1\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-gpu"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:32:19.507353Z",
          "iopub.execute_input": "2024-05-17T06:32:19.508182Z",
          "iopub.status.idle": "2024-05-17T06:32:34.760531Z",
          "shell.execute_reply.started": "2024-05-17T06:32:19.508143Z",
          "shell.execute_reply": "2024-05-17T06:32:34.759294Z"
        },
        "trusted": true,
        "id": "W6eDd_FJY0hc",
        "outputId": "3ae0b4ac-ea03-4c8b-ee39-2a4efbd304c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:33:00.163282Z",
          "iopub.execute_input": "2024-05-17T06:33:00.164153Z",
          "iopub.status.idle": "2024-05-17T06:33:12.690206Z",
          "shell.execute_reply.started": "2024-05-17T06:33:00.164109Z",
          "shell.execute_reply": "2024-05-17T06:33:12.688858Z"
        },
        "trusted": true,
        "id": "5qOY1wnzY0hc",
        "outputId": "65ca75ca-8c61-43ac-8d60-d4682174ec43"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (2.7.0)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "2kcL44GyY0hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Specify the individual PDF file path\n",
        "PDF_PATH_COLAB = '/kaggle/input/training-dataset/Training_Dataset.pdf'  # Adjust path accordingly\n",
        "\n",
        "# Specify FAISS database path in Google Drive\n",
        "DB_FAISS_PATH_DRIVE = '/kaggle/working/db_faiss'  # Adjust path accordingly\n",
        "\n",
        "# Create vector database\n",
        "def create_vector_db():\n",
        "    # Load a single PDF file directly\n",
        "    loader = PyPDFLoader(PDF_PATH_COLAB)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split the loaded document into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Create embeddings using a pre-trained model\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cuda'})\n",
        "    db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "    # Save FAISS database\n",
        "    db.save_local(DB_FAISS_PATH_DRIVE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_vector_db()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:33:18.132380Z",
          "iopub.execute_input": "2024-05-17T06:33:18.133226Z",
          "iopub.status.idle": "2024-05-17T06:49:21.729518Z",
          "shell.execute_reply.started": "2024-05-17T06:33:18.133190Z",
          "shell.execute_reply": "2024-05-17T06:49:21.728274Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "6fc13f7777c2449ab1df8afa3f521bf6",
            "7c6d3ff7c6314c12850b4f06a0fe5565",
            "240d6906898a4f059fbf67ce0dc8fa35",
            "82d5a9fca30c4398a06a65c2ca53d94f",
            "efb053047eda4e02b524e5f808553520",
            "a54102f3bcb84c97af7e0d442e90b1a2",
            "d78526ff82bd4f05abdf8ef945ae48f2",
            "795dbf3d8b8748c79e8a51f54be16a9e",
            "2bb73e431945492dae9d15ebb62b3faf",
            "21b25dcce23049978cf6eaceb49f0250",
            "5b9d058d5b5b438aa9651500a2b55f5b"
          ]
        },
        "id": "8Vv7v0FyY0hd",
        "outputId": "b8269718-bd78-4d19-cd4c-ec03d66a4dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fc13f7777c2449ab1df8afa3f521bf6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c6d3ff7c6314c12850b4f06a0fe5565"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "240d6906898a4f059fbf67ce0dc8fa35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82d5a9fca30c4398a06a65c2ca53d94f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efb053047eda4e02b524e5f808553520"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a54102f3bcb84c97af7e0d442e90b1a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d78526ff82bd4f05abdf8ef945ae48f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "795dbf3d8b8748c79e8a51f54be16a9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bb73e431945492dae9d15ebb62b3faf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21b25dcce23049978cf6eaceb49f0250"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b9d058d5b5b438aa9651500a2b55f5b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ctransformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T06:50:56.373003Z",
          "iopub.execute_input": "2024-05-17T06:50:56.373637Z",
          "iopub.status.idle": "2024-05-17T06:51:09.807143Z",
          "shell.execute_reply.started": "2024-05-17T06:50:56.373609Z",
          "shell.execute_reply": "2024-05-17T06:51:09.805944Z"
        },
        "trusted": true,
        "id": "IAZ_G_UWY0hd",
        "outputId": "fac8debc-a1ff-4623-d7fb-ba166c53384d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting ctransformers\n  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from ctransformers) (0.22.2)\nRequirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from ctransformers) (9.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (2024.2.2)\nDownloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ctransformers\nSuccessfully installed ctransformers-0.2.27\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG implementation"
      ],
      "metadata": {
        "id": "F8jIbyEHY0hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import CTransformers\n",
        "from langchain.chains import RetrievalQA\n",
        "from IPython.display import display, FileLink\n",
        "\n",
        "# Specifying the paths\n",
        "PDF_PATH_COLAB = '/kaggle/input/training-dataset/Training_Dataset.pdf'\n",
        "\n",
        "\n",
        "OUTPUT_DIR = '/kaggle/working/'\n",
        "\n",
        "# Specifying the filename for the FAISS database\n",
        "DB_FAISS_FILENAME = 'db_faiss.faiss'  # Adjust filename accordingly\n",
        "\n",
        "# Specifying the filename for the LLM model\n",
        "LLM_MODEL_FILENAME = 'llm_model'  # Adjust filename accordingly\n",
        "\n",
        "# Custom prompt template\n",
        "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below.\n",
        "Helpful answer:\n",
        "\"\"\"\n",
        "\n",
        "def set_custom_prompt():\n",
        "    prompt = PromptTemplate(template=custom_prompt_template, input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "def load_llm():\n",
        "    llm = CTransformers(\n",
        "        model=\"TheBloke/Llama-2-7B-Chat-GGML\",\n",
        "        model_type=\"llama\",\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.5\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "def create_vector_db():\n",
        "    # Load a single PDF file directly\n",
        "    loader = PyPDFLoader(PDF_PATH_COLAB)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Splitting\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Creating embeddings using a pre-trained model\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cuda'})\n",
        "    db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "    # Saving FAISS database\n",
        "    faiss_db_path = OUTPUT_DIR + DB_FAISS_FILENAME\n",
        "    db.save_local(faiss_db_path)\n",
        "\n",
        "    return faiss_db_path\n",
        "\n",
        "def retrieval_qa_chain(llm, prompt, db):\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type='stuff',\n",
        "        retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={'prompt': prompt}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "def qa_bot():\n",
        "    # Load the LLM model\n",
        "    llm = load_llm()\n",
        "\n",
        "\n",
        "    faiss_db_path = create_vector_db()\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cuda'})\n",
        "    db = FAISS.load_local(faiss_db_path, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "    qa_prompt = set_custom_prompt()\n",
        "\n",
        "    # Creating the QA chain\n",
        "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
        "\n",
        "    return qa\n",
        "\n",
        "\n",
        "def final_result(query):\n",
        "    qa_result = qa_bot()\n",
        "    response = qa_result({'query': query})\n",
        "    return response\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"What is the recommended treatment for hypertension?\"\n",
        "    result = final_result(query)\n",
        "    print(result)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-17T07:13:10.650877Z",
          "iopub.execute_input": "2024-05-17T07:13:10.651516Z",
          "iopub.status.idle": "2024-05-17T07:30:20.074240Z",
          "shell.execute_reply.started": "2024-05-17T07:13:10.651488Z",
          "shell.execute_reply": "2024-05-17T07:30:20.073258Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "d839f107c66d485880a3a895f8d4da53",
            "f4ec5512579a484d88f8b850661c85b9"
          ]
        },
        "id": "VVvztyQ6Y0hd",
        "outputId": "a6eaf1e5-4031-4b6c-f08b-f97eec25f592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d839f107c66d485880a3a895f8d4da53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4ec5512579a484d88f8b850661c85b9"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "{'query': 'What is the recommended treatment for hypertension?', 'result': \"I don't know of any natural ways to treat hypertension. The best approach to treating hypertension is through a combination of lifestyle changes and medication. A low-salt diet, regular exercise, stress reduction techniques, and weight loss can all help to lower blood pressure. Medications such as telmisartan and hydrochlorothiazide are often prescribed to treat high blood pressure.\\n\\nPlease let me know if there is anything else I can assist you with.\", 'source_documents': [Document(page_content='Answer: there are very good treatment available for hypertension first one should consume low salt diet i usually start the patient on telmisartan hydrochlorothiazide combination after ruling out contraindications and titrate for response we can add amlodipine or other drugs if needed', metadata={'source': '/kaggle/input/training-dataset/Training_Dataset.pdf', 'page': 3729}), Document(page_content='Question: description q i have hypertension please suggest me some natural ways to treat this patient hi doctor i have hypertension please suggest some natural ways to treat it', metadata={'source': '/kaggle/input/training-dataset/Training_Dataset.pdf', 'page': 2135})]}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "fGcv923kY0he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import random\n",
        "\n",
        "# Individual CSV file path\n",
        "CSV_PATH = '/kaggle/input/training-dataset/Testing_Dataset.pdf'  # Adjust the path to your CSV file\n",
        "\n",
        "# FAISS database path\n",
        "DB_FAISS_PATH_DRIVE = '/kaggle/working/db_faiss'  # Adjust path accordingly\n",
        "\n",
        "# Columns in the CSV file that contain the question and answer data\n",
        "QUESTION_COLUMN = 'Question'  # Adjust the column name accordingly\n",
        "ANSWER_COLUMN = 'Answer'  # Adjust the column name accordingly\n",
        "\n",
        "# Creating vector database\n",
        "def create_vector_db():\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    # Randomly sample two rows\n",
        "    sampled_df = df.sample(n=2, random_state=1)\n",
        "\n",
        "    # Extracting the question and answer data from the specified columns\n",
        "    questions = sampled_df[QUESTION_COLUMN].dropna().tolist()  # Drop any NaN values and convert to a list\n",
        "    answers = sampled_df[ANSWER_COLUMN].dropna().tolist()  # Drop any NaN values and convert to a list\n",
        "\n",
        "    # Splitting the question and answer data into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "    question_documents = []\n",
        "    for question in questions:\n",
        "        splitted_texts = text_splitter.split_text(question)\n",
        "        for split_text in splitted_texts:\n",
        "            question_documents.append(Document(page_content=split_text))\n",
        "\n",
        "    answer_documents = []\n",
        "    for answer in answers:\n",
        "        splitted_texts = text_splitter.split_text(answer)\n",
        "        for split_text in splitted_texts:\n",
        "            answer_documents.append(Document(page_content=split_text))\n",
        "\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
        "    question_db = FAISS.from_documents(question_documents, embeddings)\n",
        "    answer_db = FAISS.from_documents(answer_documents, embeddings)\n",
        "\n",
        "    # Save FAISS database\n",
        "    question_db.save_local(DB_FAISS_PATH_DRIVE + '_questions')\n",
        "    answer_db.save_local(DB_FAISS_PATH_DRIVE + '_answers')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_vector_db()\n"
      ],
      "metadata": {
        "id": "v_0PrwQ3Y0he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import CTransformers\n",
        "from langchain.chains import RetrievalQA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Paths and filenames\n",
        "DB_FAISS_QUESTIONS_PATH = '/kaggle/working/db_faiss_questions'\n",
        "DB_FAISS_ANSWERS_PATH = '/kaggle/working/db_faiss_answers'\n",
        "\n",
        "# Custom prompt template\n",
        "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below.\n",
        "Helpful answer:\n",
        "\"\"\"\n",
        "\n",
        "def set_custom_prompt():\n",
        "    prompt = PromptTemplate(template=custom_prompt_template, input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "def load_llm():\n",
        "    llm = CTransformers(\n",
        "        model=\"TheBloke/Llama-2-7B-Chat-GGML\",\n",
        "        model_type=\"llama\",\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.5\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "def retrieval_qa_chain(llm, prompt, question_db, answer_db):\n",
        "    # Use the question_db to retrieve relevant question embeddings\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type='stuff',\n",
        "        retriever=question_db.as_retriever(search_kwargs={'k': 2}),\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={'prompt': prompt}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "def qa_bot():\n",
        "    # Load the LLM model\n",
        "    llm = load_llm()\n",
        "\n",
        "    # Load the FAISS databases for questions and answers\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cuda'})\n",
        "    question_db = FAISS.load_local(DB_FAISS_QUESTIONS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "    answer_db = FAISS.load_local(DB_FAISS_ANSWERS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "    qa_prompt = set_custom_prompt()\n",
        "\n",
        "    # Create the QA chain\n",
        "    qa = retrieval_qa_chain(llm, qa_prompt, question_db, answer_db)\n",
        "\n",
        "    return qa, question_db, answer_db\n",
        "\n",
        "def compute_cosine_similarity(generated_answer, reference_answer):\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "    # Encode generated and reference answers\n",
        "    generated_embedding = model.encode(generated_answer)\n",
        "    reference_embedding = model.encode(reference_answer)\n",
        "\n",
        "    # Compute cosine similarity between embeddings\n",
        "    similarity_score = cosine_similarity([generated_embedding], [reference_embedding])[0][0]\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "def evaluate_rag(queries, expected_answers, threshold=0.7):\n",
        "    generated_answers = []\n",
        "    qa, question_db, answer_db = qa_bot()\n",
        "\n",
        "    # Encode expected answers\n",
        "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    expected_answer_embeddings = model.encode(expected_answers)\n",
        "\n",
        "    for query in queries:\n",
        "        response = qa({'query': query})\n",
        "        generated_answers.append(response['result'])\n",
        "\n",
        "    # Encode generated answers\n",
        "    generated_answer_embeddings = model.encode(generated_answers)\n",
        "\n",
        "    # Compute cosine similarity between question-answer pairs\n",
        "    similarity_scores = cosine_similarity(generated_answer_embeddings, expected_answer_embeddings)\n",
        "\n",
        "    # Convert similarity scores to binary results based on a threshold\n",
        "    binary_results = similarity_scores.diagonal() >= threshold\n",
        "\n",
        "    accuracy = binary_results.mean()\n",
        "    precision = binary_results.sum() / len(binary_results)\n",
        "    recall = binary_results.sum() / len(binary_results)\n",
        "\n",
        "\n",
        "    # Log the results for inspection\n",
        "    for i, (query, expected, generated, score) in enumerate(zip(queries, expected_answers, generated_answers, similarity_scores.diagonal())):\n",
        "        print(f\"Query {i+1}: {query}\")\n",
        "        print(f\"Expected Answer: {expected}\")\n",
        "        print(f\"Generated Answer: {generated}\")\n",
        "        print(f\"Similarity Score: {score}\")\n",
        "        print()\n",
        "\n",
        "    return accuracy, precision, recall, f1, similarity_scores\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your test data from CSV\n",
        "    CSV_PATH = '/content/drive/MyDrive/Testing_Dataset.csv'\n",
        "    test_data = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    # Sampling the dataframe\n",
        "    test_data = test_data.sample(n=2, random_state=42)\n",
        "\n",
        "\n",
        "    queries = test_data['Question'].tolist()\n",
        "    expected_answers = test_data['Answer'].tolist()\n",
        "\n",
        "    # Evaluating the RAG method with the threshold value of 0.7\n",
        "    accuracy, precision, recall, f1, similarity_scores = evaluate_rag(queries, expected_answers, threshold=0.7)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision * 100:.2f}%\")\n",
        "    print(f\"Recall: {recall * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lnbRqVIgY0he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interface"
      ],
      "metadata": {
        "id": "tg_4QxkuY0he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define your previous function for processing queries\n",
        "def final_result(query):\n",
        "    # Your previous code for processing the query and generating a response\n",
        "    response = \"This is a placeholder response for the query: '{}'\".format(query)\n",
        "    return response\n",
        "\n",
        "# Define the Gradio interface function\n",
        "def interface(query):\n",
        "    # Call the previous function to get the response\n",
        "    response = final_result(query)\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(fn=interface, inputs=\"text\", outputs=\"text\", title=\"Health-Bot\", description=\"Ask any medical related question\")\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "f2Tm6u1TY0he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T18:14:11.579111Z",
          "iopub.execute_input": "2024-06-03T18:14:11.579518Z",
          "iopub.status.idle": "2024-06-03T18:16:21.764216Z",
          "shell.execute_reply.started": "2024-06-03T18:14:11.579469Z",
          "shell.execute_reply": "2024-06-03T18:16:21.762774Z"
        },
        "trusted": true,
        "id": "t6AgHE1zY0he",
        "outputId": "64acb4ca-1a6e-44cb-9367-eadd60decc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d197e3ac280>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d197e3ac430>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d197e3ac9d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d197e3acb80>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7d197e3acd30>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/gradio/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement gradio (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for gradio\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}